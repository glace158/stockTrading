model: roberta-tokenized-gpt2
task:
  name: CommonGEN
training:
  batch_size: 16
  evaluation:
    batch_size: 16
  gradient_accumulation_batches: 1
  optimization:
    optimizer_name: AdamW
    weight_decay: 0.01
  random_seed: 123
  scheduler:
    scheduler_name: WarmupLinear
    warmup_steps: 0
w:
- x
- y
- z
