env_name:
  max: None
  min: None
  note: environment name
  value: RichDog
has_continuous_action_space:
  max: None
  min: None
  note: continuous action space; else discrete (연속 환경 유무)
  value: 'True'
random_search:
  max: None
  min: None
  note: random parameter traing(랜덤 파라미터 학습)
  value: '0'
cnn_features_dim:
  max: None
  min: None
  note: CNN Output features dim(CNN 출력층 크기)
  value: '256'
mlp_features_dim:
  max: None
  min: None
  note: MLP Output features dim(MLP 출력층 크기)
  value: '32'
max_training_timesteps:
  max: None
  min: None
  note: break training loop if timeteps > max_training_timesteps (총 학습 타임스텝)
  value: '10000000'
max_ep_len:
  max: None
  min: None
  note: max timesteps in one episode (에피소드 당 최대 타임 스텝)
  value: '361'
K_epochs:
  max: '10'
  min: '3'
  note: update policy for K epochs in one PPO update (최적화 횟수)
  value: '10'
update_timestep:
  max: '1440'
  min: '720'
  note: (Defult = max_ep_len * 4) update policy every n timesteps (정책 업데이트 주기)
  value: '1440'
value_loss_coef:
  max: '1.0'
  min: '0.3'
  note: 가치 손실 계수
  value: '0.5'
entropy_coef:
  max: '0.2'
  min: '0.01'
  note: 엔트로피 계수
  value: '0.01'
eps_clip:
  max: '0.3'
  min: '0.01'
  note: clip parameter for PPO (클리핑)
  value: '0.2'
gamma:
  max: '0.99'
  min: '0.99'
  note: discount factor (감가율)
  value: '0.99'
lamda:
  max: '0.95'
  min: '0.95'
  note: Advantage discount factor (어드벤티지 감가율)
  value: '0.95'
lr_actor:
  max: '0.0003'
  min: '0.00005'
  note: learning rate for actor network (액터의 학습률)
  value: '0.0003'
lr_critic:
  max: '0.003'
  min: '0.00005'
  note: learning rate for critic network (크리틱 학습률)
  value: '0.001'
action_std_method:
  max: 'None'
  min: 'None'
  note: method of decay (freq or schedule) (행동 표준 편차 방식 지정)
  value: 'schedule'
action_std:
  max: '0.4'
  min: '0.7'
  note: starting std for action distribution (Multivariate Normal) (행동 표준 편차)
  value: '0.6'
action_std_decay_freq:
  max: '1000000'
  min: '100000'
  note: action_std decay frequency (in num timesteps) (표준 편차 감소 주기)
  value: '250000'
min_action_std:
  max: '0.3'
  min: '0.05'
  note: minimum action_std (stop decay after action_std <= min_action_std) (0.05 ~
    0.1) (최소 행동 표준 편차 값)
  value: '0.1'
minibatchsize:
  max: '128'
  min: '32'
  note: mini batch size
  value: '128'
log_freq:
  max: None
  min: None
  note: (Defult / max_ep_len * 2)log avg reward in the interval (in num timesteps)
    (로그 파일 생성 주기)
  value: '2000'
print_freq:
  max: None
  min: None
  note: (Defult / max_ep_len * 10)print avg reward in the interval (in num timesteps)
    (출력 주기)
  value: '3600'
random_seed:
  max: None
  min: None
  note: set random seed if required (0 = no random seed) (랜덤 시드)
  value: '0'
render:
  max: None
  min: None
  note: environment visual rendering
  value: 'True'
save_model_freq:
  max: None
  min: None
  note: save model frequency (in num timesteps) (모델 저장 주기)
  value: '100000'
total_test_episodes:
  max: None
  min: None
  note: total num of testing episodes (테스트 횟수)
  value: '3'
frame_delay:
  max: None
  min: None
  note: Time delay per frame
  value: '0'
