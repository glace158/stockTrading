K_epochs:
  note: update policy for K epochs in one PPO update (최적화 횟수)
  value: '10'
action_std:
  note: starting std for action distribution (Multivariate Normal) (행동 표준 편차)
  value: '0.6'
action_std_decay_freq:
  note: action_std decay frequency (in num timesteps) (표준 편차 감소 주기)
  value: '500000'
count:
  note: count of data to use per training
  value: '360'
entropy_coef:
  note: 엔트로피 계수
  value: '0.01'
env_name:
  note: environment name
  value: Richdog
eps_clip:
  note: clip parameter for PPO (클리핑)
  value: '0.2'
frame_delay:
  note: Time delay per frame
  value: '0'
gamma:
  note: discount factor (감가율)
  value: '0.99'
has_continuous_action_space:
  note: continuous action space; else discrete (연속 환경 유무)
  value: 'True'
lamda:
  note: Advantage discount factor (어드벤티지 감가율)
  value: '0.95'
log_freq:
  note: (Defult / max_ep_len * 2)log avg reward in the interval (in num timesteps)
    (로그 파일 생성 주기)
  value: '2000'
lr_actor:
  note: learning rate for actor network (액터의 학습률)
  value: '0.0003'
lr_critic:
  note: learning rate for critic network (크리틱 학습률)
  value: '0.001'
max_dt:
  note: end of the data
  value: '20250120'
max_ep_len:
  note: max timesteps in one episode (에피소드 당 최대 타임 스텝)
  value: '400'
max_training_timesteps:
  note: break training loop if timeteps > max_training_timesteps (총 학습 타임스텝)
  value: '10000000'
min_action_std:
  note: minimum action_std (stop decay after action_std <= min_action_std) (0.05 ~
    0.1) (최소 행동 표준 편차 값)
  value: '0.1'
min_dt:
  note: start of the data
  value: '20160711'
minibatchsize:
  note: mini batch size
  value: '32'
print_freq:
  note: (Defult / max_ep_len * 10)print avg reward in the interval (in num timesteps)
    (출력 주기)
  value: '10000'
random_seed:
  note: set random seed if required (0 = no random seed) (랜덤 시드)
  value: '0'
render:
  note: environment visual rendering
  value: 'True'
save_model_freq:
  note: save model frequency (in num timesteps) (모델 저장 주기)
  value: '100000'
stock_code_path:
  note: the path of folder where the data is located
  value: API/datas
total_test_episodes:
  note: total num of testing episodes (테스트 횟수)
  value: '1'
update_timestep:
  note: (Defult = max_ep_len * 4) update policy every n timesteps (정책 업데이트 주기)
  value: '1600'
value_loss_coef:
  note: 가치 손실 계수
  value: '0.5'
